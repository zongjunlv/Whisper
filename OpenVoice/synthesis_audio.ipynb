{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\17655\\anaconda3\\envs\\openvoice\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\17655\\anaconda3\\envs\\openvoice\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\OpenVoice\n",
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional, Any\n",
    "\n",
    "import torch\n",
    "\n",
    "from MeloTTS.melo.api import TTS\n",
    "from openvoice import se_extractor\n",
    "from openvoice.api import ToneColorConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(\n",
    "    ckpt_converter: str,\n",
    "    device: torch.device,\n",
    "    language: str = \"ZH\",\n",
    "    local_dir=None,\n",
    "    wav_dir=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    加载模型: 1. 音色抽取和合成模型; 2. 文字转语音模型\n",
    "    转音色的功能已注释，如需要则取消注释\n",
    "    :param: ckpt_converter: 音色抽取和合成模型的权重文件\n",
    "    :param: device: 模型推理时使用的硬件设备\n",
    "    :param: language: 设定合成音频时的语言\n",
    "    \"\"\"\n",
    "    tone_color_converter = ToneColorConverter(f'{ckpt_converter}/config.json', device=device, wav_dir=wav_dir)\n",
    "    tone_color_converter.load_ckpt(f'{ckpt_converter}/checkpoint.pth')\n",
    "\n",
    "    tts_model = TTS(language, device=device, local_dir=local_dir)\n",
    "\n",
    "    return tone_color_converter, tts_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载模型中......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\17655\\anaconda3\\envs\\openvoice\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint 'D:\\OpenVoice\\checkpoints_v2/converter/checkpoint.pth'\n",
      "missing/unexpected keys: [] []\n",
      "成功加载.\n"
     ]
    }
   ],
   "source": [
    "root_path = r\"D:\\OpenVoice\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "print(\"加载模型中......\")\n",
    "wav_relative_p = \"./models--M4869--WavMark/snapshots/0ad3c7b74f641bddb61f6b85cdf2de0d93a5bfef/step59000_snr39.99_pesq4.35_BERP_none0.30_mean1.81_std1.81.model.pkl\"\n",
    "tone_color_converter, tts_model = load_model(\n",
    "    os.path.join(root_path, \"checkpoints_v2/converter\"), device=device,\n",
    "    local_dir=os.path.join(root_path, \".cache\"), wav_dir=os.path.join(root_path, \".cache\", wav_relative_p),\n",
    ")\n",
    "print(\"成功加载.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVoice version: v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating duration from bitrate, this may be inaccurate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real voice loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\17655\\anaconda3\\envs\\openvoice\\lib\\site-packages\\torch\\functional.py:650: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\SpectralOps.cpp:868.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "reference_speaker = os.path.join(root_path, \"resources/demo_speaker2.mp3\") # This is the voice you want to clone\n",
    "target_se, audio_name = se_extractor.get_se(reference_speaker, tone_color_converter, vad=False, cache_dir=os.path.join(root_path, \".cache\"))\n",
    "source_se = torch.load(os.path.join(root_path, f'checkpoints_v2/base_speakers/ses/en-newest.pth'), map_location=device)\n",
    "print(\"Real voice loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesis_toncvtr_from_file(\n",
    "    model: Optional[TTS],\n",
    "    tone_color_converter: Optional[ToneColorConverter],\n",
    "    source_se: Any, \n",
    "    target_se: Any,\n",
    "    text_file_path: str,\n",
    "    output_dir: str,\n",
    "    speed: float = 1.0\n",
    "):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    speaker_ids = model.hps.data.spk2id\n",
    "    default_speaker_id = list(speaker_ids.values())[0]\n",
    "    \n",
    "    with open(text_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line_num, text in enumerate(file.readlines(), start=1):\n",
    "            text = text.strip()  # Remove trailing newline and whitespace\n",
    "            if not text:  # Skip empty lines\n",
    "                continue\n",
    "            \n",
    "            # Generate audio using the TTS model\n",
    "            temp_audio_path = os.path.join(output_dir, f'temp_audio_{line_num}.wav')\n",
    "            model.tts_to_file(text, default_speaker_id, speed=speed, output_path=temp_audio_path)\n",
    "            \n",
    "            # Run the tone color converter\n",
    "            encode_message = \"@MyShell\"\n",
    "            output_audio_path = os.path.join(output_dir, f'output_audio_{line_num}.wav')\n",
    "            tone_color_converter.convert(\n",
    "                audio_src_path=temp_audio_path,\n",
    "                src_se=source_se,\n",
    "                tgt_se=target_se,\n",
    "                message=encode_message,\n",
    "                orig_sr=model.hps.data.sampling_rate,\n",
    "                output_path=output_audio_path\n",
    "            )\n",
    "            \n",
    "            # Optionally, delete the temporary audio file if no longer needed\n",
    "            os.remove(temp_audio_path)\n",
    "            \n",
    "            print(f\"Processed line {line_num}: '{text}' -> {output_audio_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "杭电科研实力雄厚, 在众多领域参与并完成了一系列国家“六五”至“十三五”计划重点攻关、“973”“863”等高科技攻关和国家、省部基金科研项目.\n",
      "2023年实到科研经费突破 7亿元.\n",
      "近年来, 学校获国家科技进步二等奖4项、国家发明二等奖2项、国家自然科学奖二等奖1项,\n",
      "其中, 2021年主持获得国家科技进步二等奖1项,\n",
      "2021年主持获得浙江省科学技术一等奖3项.\n",
      "学校入选教育部“高等学校科技成果转化和技术转移基地”、科技部“赋予科研人员职务科技成果所有权或长期使用权试点单位”,\n",
      "以及国家知识产权局、教育部“国家知识产权试点示范高校”,\n",
      "荣获“全国信息产业科技创新先进集体”称号.\n",
      "拥有浙江省智慧城市研究中心、浙江省“2011协同创新中心”、浙江省信息化与经济社会发展研究中心、浙江省哲学社科重点研究基地、浙江高等教育研究院、海洋工程研究中心、微电子研究中心、中国财务云服务研究院、生物三维打印与医疗器械研究院、中国科教评价研究院、浙江省管理会计应用创新研究中心、浙江(杭电)创新材料研究院等一批科技教育研究平台.\n",
      "目前, 学校与国内外数百家企业建立了稳定的科技合作关系,\n",
      "已成为浙江省科技创新与成果转化的高地,\n",
      "取得了良好经济效益和社会效益.\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:03<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed line 1: '杭电科研实力雄厚，在众多领域参与并完成了一系列国家“六五”至“十三五”计划重点攻关、“973”“863”等高科技攻关和国家、省部基金科研项目。2023年实到科研经费突破 7亿元。近年来，学校获国家科技进步二等奖4项、国家发明二等奖2项、国家自然科学奖二等奖1项，其中，2021年主持获得国家科技进步二等奖1项，2021年主持获得浙江省科学技术一等奖3项。学校入选教育部“高等学校科技成果转化和技术转移基地”、科技部“赋予科研人员职务科技成果所有权或长期使用权试点单位”，以及国家知识产权局、教育部“国家知识产权试点示范高校”，荣获“全国信息产业科技创新先进集体”称号。拥有浙江省智慧城市研究中心、浙江省“2011协同创新中心”、浙江省信息化与经济社会发展研究中心、浙江省哲学社科重点研究基地、浙江高等教育研究院、海洋工程研究中心、微电子研究中心、中国财务云服务研究院、生物三维打印与医疗器械研究院、中国科教评价研究院、浙江省管理会计应用创新研究中心、浙江(杭电)创新材料研究院等一批科技教育研究平台。目前，学校与国内外数百家企业建立了稳定的科技合作关系，已成为浙江省科技创新与成果转化的高地，取得了良好经济效益和社会效益。' -> D:\\OpenVoice\\output_audio\\output_audio_1.wav\n",
      " > Text split to sentences.\n",
      "杭电学风浓郁, 学科竞赛成绩突出,\n",
      "近五年在挑战杯、互联网+、电子设计、数学建模、ACM程序设计和智能汽车等全国大学生顶级权威学科竞赛中获得国家级二等奖以上600余项,\n",
      "8次入围ACM国际大学生程序设计大赛全球总决赛,\n",
      "曾获美国数学建模竞赛特等奖等国际奖项,\n",
      "参加省级以上学科竞赛获奖人数在省属高校中位列首位.\n",
      "2017年第41届ACM国际大学生程序设计竞赛全球总决赛获得全球排名并列第20；2017年至2022年全国大学生智能汽车竞赛全国总决赛共获一等奖36项（含6项全国冠军）,\n",
      "总成绩多年居全国第一；2020年全国大学生电子设计竞赛信息科技前沿专题邀请赛获赛事最高奖瑞萨杯；2018年全国大学生数学建模竞赛满额获奖,\n",
      "总成绩并列全国第一, 2021年斩获数学建模国赛三大奖项之一的“知网研学奖”,\n",
      "这是我省首次获得该奖项；2018年和2022年两次取得中国大学生服务外包创新创业大赛一等奖获奖数量全国第一；在2022年全国普通高校大学生竞赛七轮总榜单（本科）中排名全国第16位,\n",
      "地方本科院校竞赛榜单列全国第一.\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:01,  5.55it/s]\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\17655/nltk_data'\n    - 'c:\\\\Users\\\\17655\\\\anaconda3\\\\envs\\\\openvoice\\\\nltk_data'\n    - 'c:\\\\Users\\\\17655\\\\anaconda3\\\\envs\\\\openvoice\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\17655\\\\anaconda3\\\\envs\\\\openvoice\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\17655\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m text_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOpenVoice\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124manswer_rest.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your text file path\u001b[39;00m\n\u001b[0;32m      2\u001b[0m output_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOpenVoice\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124moutput_audio\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your desired output directory path\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43msynthesis_toncvtr_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtts_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtone_color_converter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtone_color_converter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_se\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_se\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_se\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_se\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m     \u001b[49m\u001b[43mtext_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 24\u001b[0m, in \u001b[0;36msynthesis_toncvtr_from_file\u001b[1;34m(model, tone_color_converter, source_se, target_se, text_file_path, output_dir, speed)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Generate audio using the TTS model\u001b[39;00m\n\u001b[0;32m     23\u001b[0m temp_audio_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_audio_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mline_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtts_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_speaker_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp_audio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Run the tone color converter\u001b[39;00m\n\u001b[0;32m     27\u001b[0m encode_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@MyShell\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32md:\\OpenVoice\\MeloTTS\\melo\\api.py:100\u001b[0m, in \u001b[0;36mTTS.tts_to_file\u001b[1;34m(self, text, speaker_id, output_path, sdp_ratio, noise_scale, noise_scale_w, speed, pbar, format, position, quiet)\u001b[0m\n\u001b[0;32m     98\u001b[0m     t \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([a-z])([A-Z])\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m, t)\n\u001b[0;32m     99\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m--> 100\u001b[0m bert, ja_bert, phones, tones, lang_ids \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_for_tts_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymbol_to_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    102\u001b[0m     x_tst \u001b[38;5;241m=\u001b[39m phones\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32md:\\OpenVoice\\MeloTTS\\melo\\utils.py:23\u001b[0m, in \u001b[0;36mget_text_for_tts_infer\u001b[1;34m(text, language_str, hps, device, symbol_to_id)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_text_for_tts_infer\u001b[39m(text, language_str, hps, device, symbol_to_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     norm_text, phone, tone, word2ph \u001b[38;5;241m=\u001b[39m \u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     phone, tone, language \u001b[38;5;241m=\u001b[39m cleaned_text_to_sequence(phone, tone, language_str, symbol_to_id)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hps\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39madd_blank:\n",
      "File \u001b[1;32md:\\OpenVoice\\MeloTTS\\melo\\text\\cleaner.py:14\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     12\u001b[0m language_module \u001b[38;5;241m=\u001b[39m language_module_map[language]\n\u001b[0;32m     13\u001b[0m norm_text \u001b[38;5;241m=\u001b[39m language_module\u001b[38;5;241m.\u001b[39mtext_normalize(text)\n\u001b[1;32m---> 14\u001b[0m phones, tones, word2ph \u001b[38;5;241m=\u001b[39m \u001b[43mlanguage_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mg2p\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m norm_text, phones, tones, word2ph\n",
      "File \u001b[1;32md:\\OpenVoice\\MeloTTS\\melo\\text\\chinese_mix.py:81\u001b[0m, in \u001b[0;36mg2p\u001b[1;34m(text, impl)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n\u001b[1;32m---> 81\u001b[0m phones, tones, word2ph \u001b[38;5;241m=\u001b[39m \u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28msum\u001b[39m(word2ph) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(phones)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# assert len(word2ph) == len(text)  # Sometimes it will crash,you can add a try-catch.\u001b[39;00m\n",
      "File \u001b[1;32md:\\OpenVoice\\MeloTTS\\melo\\text\\chinese_mix.py:224\u001b[0m, in \u001b[0;36m_g2p_v2\u001b[1;34m(segments)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[a-zA-Z\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]+\u001b[39m\u001b[38;5;124m'\u001b[39m, text):\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# english\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     tokenized_en \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n\u001b[1;32m--> 224\u001b[0m     phones_en, tones_en, word2ph_en \u001b[38;5;241m=\u001b[39m \u001b[43mg2p_en\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_start_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_en\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;66;03m# apply offset to tones_en\u001b[39;00m\n\u001b[0;32m    226\u001b[0m     tones_en \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;241m+\u001b[39m language_tone_start_map[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEN\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tones_en]\n",
      "File \u001b[1;32md:\\OpenVoice\\MeloTTS\\melo\\text\\english.py:244\u001b[0m, in \u001b[0;36mg2p\u001b[1;34m(text, pad_start_end, tokenized)\u001b[0m\n\u001b[0;32m    242\u001b[0m     phone_len \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(phns)\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 244\u001b[0m     phone_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m p: p \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43m_g2p\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ph \u001b[38;5;129;01min\u001b[39;00m phone_list:\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m ph \u001b[38;5;129;01min\u001b[39;00m arpa:\n",
      "File \u001b[1;32mc:\\Users\\17655\\anaconda3\\envs\\openvoice\\lib\\site-packages\\g2p_en\\g2p.py:162\u001b[0m, in \u001b[0;36mG2p.__call__\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# tokenization\u001b[39;00m\n\u001b[0;32m    161\u001b[0m words \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[1;32m--> 162\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# tuples of (word, tag)\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# steps\u001b[39;00m\n\u001b[0;32m    165\u001b[0m prons \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\17655\\anaconda3\\envs\\openvoice\\lib\\site-packages\\nltk\\tag\\__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32mc:\\Users\\17655\\anaconda3\\envs\\openvoice\\lib\\site-packages\\nltk\\tag\\__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m PerceptronTagger(lang\u001b[38;5;241m=\u001b[39mlang)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32mc:\\Users\\17655\\anaconda3\\envs\\openvoice\\lib\\site-packages\\nltk\\tag\\perceptron.py:183\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load, lang)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\17655\\anaconda3\\envs\\openvoice\\lib\\site-packages\\nltk\\tag\\perceptron.py:273\u001b[0m, in \u001b[0;36mPerceptronTagger.load_from_json\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc \u001b[38;5;241m+\u001b[39m TAGGER_JSONS[lang][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fin)\n",
      "File \u001b[1;32mc:\\Users\\17655\\anaconda3\\envs\\openvoice\\lib\\site-packages\\nltk\\data.py:582\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    580\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    581\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 582\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\17655/nltk_data'\n    - 'c:\\\\Users\\\\17655\\\\anaconda3\\\\envs\\\\openvoice\\\\nltk_data'\n    - 'c:\\\\Users\\\\17655\\\\anaconda3\\\\envs\\\\openvoice\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\17655\\\\anaconda3\\\\envs\\\\openvoice\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\17655\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "text_file = r\"D:\\OpenVoice\\answer_rest.txt\"  # Replace with your text file path\n",
    "output_directory = r\"D:\\OpenVoice\\output_audio\"  # Replace with your desired output directory path\n",
    "synthesis_toncvtr_from_file(\n",
    "    model=tts_model,\n",
    "    tone_color_converter=tone_color_converter,\n",
    "    source_se=source_se,\n",
    "    target_se=target_se,\n",
    "     text_file_path=text_file,\n",
    "    output_dir=output_directory,\n",
    "    speed=1.0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvoice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
